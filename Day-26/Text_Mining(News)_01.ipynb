{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝과정\n",
    "- 1. 수집<br>\n",
    "    1) 다운로드<br>\n",
    "    2) 크롤링\n",
    "\n",
    "\n",
    "- 2. 전처리 : 컴퓨터가 이해하지 못 하는 텍스트의 특징을 알 수 있게 해줌.<br>\n",
    "    1) 토큰화  : 텍스트를 정해진 단위로 나눔 = 초성 종성 등.<br>\n",
    "    2) 불용어 처리 : 몇 개를 지움 = Stopwords<br>\n",
    "    3) 대소문자 통일 <br>\n",
    "    4) 어근 추출 : 의미가 있는 단어 통일 = played, playing, plays에서 play<br>\n",
    "    5) 텍스트 인코딩 : pp 와 같은 단어를 1, 0 의 숫자로 변환\n",
    "\n",
    "\n",
    "- 3. 텍스트 분석 <br>\n",
    "    1) 토픽 모델링 : 문서들이 갖고 있는 여러가지 토픽을 찾아주는 기법<br>\n",
    "    2) 감정분석 : 텍스트에 포함된 저자의 의견을 찾아내는 기법 = 긍정 또는 부정<br>\n",
    "    3) 텍스트 분류 : 머신러닝이나 딥러닝을 통해 지정한 카테고리로 분류\n",
    "    \n",
    "    \n",
    "- 4. 텍스트 시각화<br>\n",
    "    1) 워드클라우드<br>\n",
    "    2) 의미 연결망 분석 : 그래프의 노드에 단어를, 노드 간 연결에 단어 사이의 관계를 정의하여 연결.\n",
    "        > 문서에서 등장하는 키워드 간의 관계를 분석하기 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 저장소(영어)\n",
    "- https://www.kaggle.com/datasets\n",
    "    > 데이터 과학 competition \n",
    "- https://archive.ics.uci.edu/ml/index.php\n",
    "    > UC Irvine 대학의 machine learning을 위한 데이터 저장소\n",
    "- http://ana.cachopo.org/datasets-for-single-label-text-categorization\n",
    "    > 논문 연구에서 사용할 수 있는 분류용 테스트 데이터\n",
    "- https://course.fast.ai/datasets\n",
    "    > Fast.ai에서 제공하는 딥러닝 학습용 대용량 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 저장소(한국어)\n",
    "- https://korquad.github.io\n",
    "    > LG CNS에서 공개한 한국어 질의응답 데이터\n",
    "- https://ithub.korean.go.kr/user/guide/corpus/guide1.do\n",
    "    > 국립국어원에서 공개한 세종 코퍼스\n",
    "- https://konlpy-ko.readthedocs.io/ko/v0.5.1/data\n",
    "    > 파이썬 한국어 처리 라이브러리 konlpy에서 제공하는 데이터\n",
    "- http://aiopen.etri.re.kr/service_dataset.php\n",
    "    > ETR에서 공개한 언어처리 학습데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 워드임베딩이란?\n",
    "- 단어를 컴퓨터가 이해할 수 있는 벡터로 표현하는 방법.\n",
    "\n",
    "\n",
    "- Word Embedding = Word + Embedding\n",
    "      >Sparase Representation (BOW, TF-IDF)\n",
    "      >Dense Representation (word2vec, Glove 등)\n",
    "- 딥러닝 기법에서 워드 임베딩이 입력값으로 들어감.\n",
    "\n",
    "\n",
    "### Sparse Representation (희소 표현)의 문제점\n",
    "- 문서 데이터에 존재하는 모든 유니크한 단어 수가 벡터의 차원이 되어 고차원 공간이 됨.\n",
    "- 단어의 문맥 정보가 사라짐 ex) 문장 내 순서, 문장 내 동시 등장\n",
    "- 차원의 저주로 인해 분석 기법의 성능이 악화됨.\n",
    "\n",
    "\n",
    "### Dense Representation (밀집 표현)\n",
    "- 이미지나 오디오 데이터는 양질의 고차원 데이터로 표현됨\n",
    "- 기존 방법인 VSM은 단어를 discrete symbol로 표시하기 때문에 정보 전달력이 떨어짐.\n",
    "- 기존의 count-based method가 아닌 predictive model을 사용하여 단어 주변 정보를 반영한 dense representation이 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import requests\n",
    "\n",
    "news_data = []\n",
    "\n",
    "client_id = \"B4MR1HUQS2l2Qwtvdyzx\"\n",
    "client_secret = \n",
    "encText = urllib.parse.quote(\"파이썬\") # 뭘 검색할 것인지\n",
    "url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText # json 결과\n",
    "# url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # xml 결과\n",
    "\n",
    "\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "\n",
    " \n",
    "if(rescode==200):\n",
    "#    response_body = response.read()\n",
    "    result = requests.get(response.geturl(),\n",
    "                         headers={'X-Naver-Client-Id':client_id,\n",
    "                                 'X-Naver-Client-Secret':client_secret})\n",
    "    news_data.append(result.json())\n",
    "# 처음에 만들어 뒀던 news_data에 dictionary 형태로 저장\n",
    "# 결과는 출력되지 않음.\n",
    "#    print(response_body.decode('utf-8'))\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 OPEN API를 활용해 데이터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://blog.naver.com/blueskyjoo7?Redirect=Log&logNo=222247271856\n"
     ]
    }
   ],
   "source": [
    "print(news_data[0]['items'][0]['link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가져온 URL이 네이버 뉴스인지 확인하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_news_link = []\n",
    "\n",
    "for item in news_data[0]['items']:\n",
    "    link = item['link']\n",
    "    if 'naver'in link :\n",
    "        page_news_link.append(link)\n",
    "        \n",
    "len(page_news_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
