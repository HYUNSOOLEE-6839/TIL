{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "### Tensorflow 2.0 사용해보기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 데이터 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0 , x_test / 255.0"
   ]
  },
  {
   "source": [
    "#### 네트워크 구조 정의"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)), tf.keras.layers.Dense(128, activation='relu'),\n",
    "tf.keras.layers.Dropout(0.2),\n",
    "tf.keras.layers.Dense(10, activation='softmax')])"
   ]
  },
  {
   "source": [
    "#### Keras 모델 Compile"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "loss = 'sparse_categorical_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "#### 학습 수행"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 427us/step - loss: 0.5096 - accuracy: 0.8477\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 419us/step - loss: 0.2001 - accuracy: 0.9408\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 411us/step - loss: 0.1544 - accuracy: 0.9548\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 412us/step - loss: 0.1328 - accuracy: 0.9607\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 416us/step - loss: 0.1247 - accuracy: 0.9629\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x124025d30>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "source": [
    "### 학습 결과 테스트"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 0s 311us/step - loss: 0.0904 - accuracy: 0.9732\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.09036891162395477, 0.9732000231742859]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "source": [
    "- 97.32%의 정확도를 보임."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 활성함수\n",
    "- 퍼셉트론의 출력에 의미를 부여해주며, 일반적으로 비선형함수이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Tanh와 Soft decision\n",
    "- 값이 작아질 수록 -1, 커질 수록 1에 수렴\n",
    "- 모든 실수 입력 값에 대해 출력이 정의됨.\n",
    "- 출력이 실수 값으로, Soft decision\n",
    "- 입력 값이 0에 가까울 수록 출력이 빠르게 변함.\n",
    "- 모든 점에서 미분 가능"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Sigmoid function\n",
    "- 값이 작아질 수록 0, 커질 수록 1에 수렴\n",
    "- 모든 실수 입력 값에 대해 출력이 정의됨.\n",
    "- 출력이 0~1 사이로, '확률'을 표현할 수 있음.\n",
    "- 입력 값이 0에 가까울 수록 출력이 빠르게 변함.\n",
    "- 모든 점에서 미분 가능\n",
    "    > Sigmoid 함수는 본래 S-Shape의 함수 모두를 가르킴."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Softmax function\n",
    "- 각 입력의 지수함수를 정규화한 것 -> 총합이 1이 되는 것.\n",
    "- 각 출력은 0~1 사이의 값을 가짐\n",
    "- 모든 출력의 합이 반드시 1이 됨\n",
    "- 여러 경우의 수 중 한가지에 속할 '확률'을 표현\n",
    "    > Softmax는 최종 출력 단에서 N가지 범주로 분류하는 Multi-class classification에 쓰임."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### ReLU\n",
    "- 0보다 작은 값을 0으로 강제하는 함수\n",
    "- 딥러닝에서 가장 많이 사용됨.\n",
    "- 미분 값이 일정(0 또는 1)해서 학습이 잘 되는 특성\n",
    "- 단순한 구현으로 매우 빠른 연산이 가능"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 손실 함수\n",
    "- 최적화 이론에서 최소화하고자 하는 함수\n",
    "- 알고리즘 학습 중 아직 '얼마나 못 하는지' 표현\n",
    "- 보통 미분 가능한 함수를 사용.\n",
    "- 목적 함수, 비용 함수, 에너지 함수 등으로 불림.\n",
    "\n",
    "1) MSE : 평균제곱오차\n",
    "    > 가장 기본적인 손실 함수. 오차가 커질 수록 손실함수가 빠르게 증가하는 특징이 있으며, \n",
    "    회귀에서 많이 쓰임.\n",
    "\n",
    "2) MAE : 평균절대오차\n",
    "    > 오차가 커져도 손실함수가 일정하게 증가하는 특징이 있다. Outlier에 강건한 특징이 있다. \n",
    "    통계적으로 중간 값과 연관이 있으며 회귀에 많이 쓰임.\n",
    "\n",
    "3) 원-핫 인코딩\n",
    "    > 다중 클래스 분류 문제의 정답을 표기할 때 사용. 정답 벡터의 각 인덱스 별로 클래스를 미리 정의하고, \n",
    "    정답에 해당하는 요소만 1로 표현, 이러한 정답 벡터를 Label이라부름.\n",
    "\n",
    "4) CCE : 교차 엔트로피 오차\n",
    "    > 원-핫 인코딩으로 인해, 정답인 클래스에 대해서만 오차를 계산. \n",
    "    정확히 맞추면 오차가 0, 틀릴 수록 오차가 무한히 증가하는 특징이 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 신경망 구조\n",
    "- 뉴런을 기본 단위로 하며, 이를 조합하여 복잡한 구조를 이룸.\n",
    "- 보통 신경망을 표현할 때, Graph의 Node와 Edge를 이용해 표현한다.\n",
    "    > Node는 단일 뉴런 연산을, Edge는 뉴런의 연결성을 의미한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Fully-Connected Layer(FCN) : 전결합계층\n",
    "- 두 계층 간의 모든 뉴런이 연결되어 있는 계층을 뜻함."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 얕은 신경망 (Shallow Neural Network)\n",
    "- 입력 계층, 은닉 계층, 출력 계층으로 이루어진 신경망\n",
    "- 심층 신경망의 등장 이후 기존의 신경망은 '얕은' 신경망으로 부름.\n",
    "- 각 계층은 Fully-connected layer로 이루어져 있음."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 심층 신경망 (DNN)\n",
    "- 얕은 신경망보다 은닉 계층이 많은 신경망\n",
    "- 보통 5개 이상의 계층이 있는 경우 '깊다' (Deep 하다) 라고 표현."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Perceptron 구현해보기 (OR 함수)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, w, b): # weight, bias\n",
    "        self.w = tf.Variable(w, dtype=tf.float32)\n",
    "        self.b = tf.Variable(b, dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, x): # perceptron을 통과했을 때 어떤 출력이 나오는지\n",
    "        return tf.sign(tf.reduce_sum(self.w * x) + self.b)"
   ]
  },
  {
   "source": [
    "### Utility 함수 구현"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *args에 알아서 Tuple 형태로 들어오게 되고, np.array로 매번 입력할 필요 없음.\n",
    "def v(*args):\n",
    "    return np.array(args)"
   ]
  },
  {
   "source": [
    "### Perceptron 정의 - weight, bias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = v(1, 1)\n",
    "b = 0.5\n",
    "\n",
    "perceptron = Perceptron(w, b)"
   ]
  },
  {
   "source": [
    "### Perceptron 동작 확인"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0 1.0\n-1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "p1 = perceptron(v(1, 1)) # 1사분면 / T, T\n",
    "p2 = perceptron(v(-1, 1)) # 2사분면 / F, T\n",
    "p3 = perceptron(v(-1, -1)) # 3사분면 / F, F\n",
    "p4 = perceptron(v(1, -1)) # 4사분면 / T, F\n",
    "\n",
    "print(p2.numpy(), p1.numpy())\n",
    "print(p3.numpy(), p4.numpy())"
   ]
  },
  {
   "source": [
    "# Perceptron 구현해보기 (XOR 함수)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Perceptron 정의"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_nand = Perceptron(w=v(-1, -1), b= 0.5)\n",
    "\n",
    "p_or = Perceptron(w=v(1, 1), b=0.5)\n",
    "\n",
    "p_and = Perceptron(w=v(1, 1), b=-0.5)\n",
    "\n",
    "def xor(x):\n",
    "    h1 = p_nand(x)\n",
    "    h2 = p_or(x)\n",
    "    return p_and(v(h1, h2))"
   ]
  },
  {
   "source": [
    "### Perceptron 동작확인"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0 -1.0\n-1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "p1 = xor(v(1, 1)) # 1사분면 / T, T\n",
    "p2 = xor(v(-1, 1)) # 2사분면 / F, T\n",
    "p3 = xor(v(-1, -1)) # 3사분면 / F, F\n",
    "p4 = xor(v(1, -1)) # 4사분면 / T, F\n",
    "\n",
    "print(p2.numpy(), p1.numpy())\n",
    "print(p3.numpy(), p4.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hs",
   "language": "python",
   "name": "hs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}