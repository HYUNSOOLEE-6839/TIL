{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b7330c6fd2538eed64e4726f7b892761768e972a75baa1127b9d2c29ee62c26b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 영상의 합성곱 계산\n",
    "- 2-D 디지털 신호의 합성은 필터를 한 칸씩 옮기면서 영상과 겹쳐지는 부분을 모두 곱해 합치면 된다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Padding의 필요성\n",
    "- 합성곱 연산 시, 필터(커널)의 크기에 따라 영상의 크기가 줄어드는 문제가 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Zero-Padding\n",
    "- 크기가 (2N + 1)인 커널에 대해, 상하좌우에 N개의 Zero-Padding을 해주면 된다.\n",
    "    > 투입과 출력의 크기를 똑같이 유지할 수 있음."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Stride\n",
    "- 합성곱 연산에서 커널을 이동시키는 거리를 Stride라고 하며, 크게 하면 출력의 크기가 줄어든다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 곱에서 합성곱으로\n",
    "- 입력 뉴런 대신 입력 영상을, 가중치 대신 필터를, 곱 대신 합성곱을 사용하면 된다.\n",
    "- 편향(Bias)은 그대로 동일하게 유지된다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 풀링 계층\n",
    "- 여러 화소를 종합하여 하나의 화소로 변환하는 계층.\n",
    "    > 풀링 계층을 통과하면 영상의 크기가 줄어들고, 정보가 종합된다.<br>\n",
    "\n",
    "- 가장 많이 쓰이는 방법은 최댓값(Max-Pooling)과 평균(Average-Pooling)이다.\n",
    "- 합성곱 신경망의 애플리케이션에 맞는 풀링 계층을 사용한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# CNN 학습 실습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### import modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "source": [
    "### 하이퍼 파라미터"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "source": [
    "### 네트워크 구조"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyModel():\n",
    "    return Sequential([Conv2D(32, (3, 3), padding='same', activation='relu'), MaxPool2D(),\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'), MaxPool2D(),\n",
    "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')]) \n",
    "    # Filter 갯수, (3,3), padding = same : Conv-layer 통해도 zero 패딩통해 그대로 유지"
   ]
  },
  {
   "source": [
    "### 데이터 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "# 차원을 하나 덧붙이기, NHWC\n",
    "x_train = x_train[..., np.newaxis] \n",
    "x_test = x_test[..., np.newaxis]\n",
    "# ...은 앞에 있는 모든 axis에 대해 전부다 포함한다는 의미임.\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(2048)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32).prefetch(2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 28, 28, 1)\n(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape) # 3차원\n",
    "print(x_train[0].shape) # 2차원\n",
    "# 3차원에 차원을 하나 덧붙여주어야함.\n",
    "# CNN을 학습할 때 데이터의 구조는 batch + height + width + channel\n",
    "# 총 랭크가 4인 데이터셋을 필요로함."
   ]
  },
  {
   "source": [
    "### 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "loss='sparse_categorical_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "### 모델 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.5407 - accuracy: 0.8026 - val_loss: 0.2836 - val_accuracy: 0.8960\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.2537 - accuracy: 0.9080 - val_loss: 0.2894 - val_accuracy: 0.8948\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.2076 - accuracy: 0.9240 - val_loss: 0.2570 - val_accuracy: 0.9083\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1727 - accuracy: 0.9348 - val_loss: 0.2497 - val_accuracy: 0.9134\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1466 - accuracy: 0.9446 - val_loss: 0.2466 - val_accuracy: 0.9215\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1156 - accuracy: 0.9573 - val_loss: 0.2604 - val_accuracy: 0.9175\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0945 - accuracy: 0.9639 - val_loss: 0.3014 - val_accuracy: 0.9150\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0785 - accuracy: 0.9702 - val_loss: 0.3078 - val_accuracy: 0.9190\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0631 - accuracy: 0.9765 - val_loss: 0.3293 - val_accuracy: 0.9207\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0533 - accuracy: 0.9802 - val_loss: 0.3834 - val_accuracy: 0.9143\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}