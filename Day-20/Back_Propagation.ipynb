{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b7330c6fd2538eed64e4726f7b892761768e972a75baa1127b9d2c29ee62c26b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Back Propagation : 역전파 알고리즘\n",
    "- 수치적 기울기 : 미분의 정의로부터 극한 연산을 근사해 수치적 기울기를 구할 수 있다.\n",
    "    > 입실론의 값이 충분히 작다면, 수치적 기울기를 미분 값으로 사용 가능.<br>\n",
    "    > 각 스칼라 변수를 조금씩 바꾸어 대입해보면서 수치적 기울기를 구한다.<br>\n",
    "    > N개의 매개 변수로 미분하기 위해 (N+1)번 더 손실함수를 평가해야한다.<br>\n",
    "\n",
    "- 심층 신경망의 수치적 기울기 \n",
    "    > Gradient Descent 한 스텝 계산을 위한 연산은 N(N + 1)번의 곱하기연산이다.<br>\n",
    "    > 10만개의 파라미터를 가진 경우 무려 100억 회가 필요하므로, 대책이 필요하다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Sigmoid 함수의 미분\n",
    "    > sigmoid(x)(1-sigmoid(x))로 표현할 수 있다.<br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 그래서 역전파 알고리즘(BP)이란?\n",
    "<br>\n",
    "    - 학습 데이터로 정방향 연산을 하여 Loss를 구한다. <br>\n",
    "    - 정방향 연산 시, 계층 별로 BP에 필요한 중간결과를 저장한다.<br>\n",
    "    - Loss를 각 파라미터로 미분한다. 연쇄 법칙(역방향 연산)을 이용한다.<br>\n",
    "        - 마지막 계층부터 하나씩 이전 계층으로 연쇄적으로 계산한다.<br>\n",
    "        - 역방향 연산 시, 정방향 연산에서 저장한 중간 결과를 사용한다.\n",
    "\n",
    "> 미분의 연쇄법칙과 각 함수의 수식적 미분을 이용하면, 단 한번의 손실함수 평가로 미분을 구할 수 있다. 단, 중간결과를 저장해야 하므로 메모리를 추가로 사용한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 활성함수의 개선 - ReLU\n",
    "- 양수랴면, 입력에 관계 없이 동일한 미분 값(1)이 나오게 하여 기울기 소실 문제를 해결.<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 수치 미분을 이용한 심층 신경망 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### import modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "### 유틸리티 함수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "\n",
    "def _t(x):\n",
    "    return np.transpose(x)\n",
    "\n",
    "def _m(A, B):\n",
    "    return np.matmul(A, B)"
   ]
  },
  {
   "source": [
    "### Sigmoid 함수 구현"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.last_o = 1\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.last_o = 1.0 / (1.0 + np.exp(-x))\n",
    "        return self.last_o\n",
    "\n",
    "    def grad(self): # sigmoid(x)(1 - sigmoid(x))\n",
    "        return self.last_o * (1.0 - self.last_o)"
   ]
  },
  {
   "source": [
    "### MSE 구현"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError: # 1/2 * mean((h - y)^2)  --> h - y\n",
    "    def __init__(self):\n",
    "        self.dh = 1\n",
    "        self.last_diff = 1\n",
    "\n",
    "    def __call__(self, h, y):\n",
    "        self.last_diff = h - y\n",
    "        return 1 / 2 * np.mean(np.square(self.last_diff))\n",
    "\n",
    "    def grad(self):\n",
    "        return self.last_diff"
   ]
  },
  {
   "source": [
    "### Dense Layer 구현"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, W, b, a_obj):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.a = a_obj()\n",
    "        \n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        self.dh = np.zeros_like(_t(self.W))\n",
    "        \n",
    "        self.last_x = np.zeros((self.W.shape[0]))\n",
    "        self.last_h = np.zeros((self.W.shape[1]))\n",
    "        \n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.last_x = x\n",
    "        self.last_h = _m(_t(self.W), x) + self.b\n",
    "        return self.a(self.last_h)\n",
    "\n",
    "    def grad(self): # dy/dh = W\n",
    "        return self.W * self.a.grad()\n",
    "\n",
    "    def grad_W(self, dh):\n",
    "        grad = np.ones_like(self.W)\n",
    "        grad_a = self.a.grad()\n",
    "        for j in range(grad.shape[1]): # dy/dw = x\n",
    "            grad[:, j] = dh[j] * grad_a[j] * self.last_x\n",
    "        return grad\n",
    "\n",
    "    def grad_b(self, dh): # dy/db = 1\n",
    "        return dh * self.a.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class Dense:\n",
    "    def __init__(self, W, b, a):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.a = a\n",
    "\n",
    "        self.dW = np.zeros_like(self.W) # w를 미분한 것\n",
    "        self.db = np.zeros_like(self.b) # b를 미분한 것\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.a(_m(_t(self.W), x) + self.b) \n",
    "        # matmul((input * output)T , i * 1) + o * 1'''"
   ]
  },
  {
   "source": [
    "- 제대로 동작하지 않아 다른 함수 정의 (추후 보완 예정)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 심층신경망 구현"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "'''class DNN:\n",
    "    def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=sigmoid):\n",
    "        def init_var(i, o):\n",
    "            return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o, ))\n",
    "        self.sequence = list()\n",
    "        # First hidden layer\n",
    "        W, b = init_var(num_input, num_neuron)\n",
    "        self.sequence.append(Dense(W, b, activation))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(hidden_depth - 1):\n",
    "            W, b = init_var(num_neuron, num_neuron)\n",
    "            self.sequence.append(Dense(W, b, activation))\n",
    "\n",
    "        # Output layer\n",
    "        W, b = init_var(num_neuron, num_output)\n",
    "        self.sequence.append(Dense(W, b, activation))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def calc_gradient(self, x, y, loss_func):\n",
    "        def get_new_sequence(layer_index, new_layer): # 값을 수정해서 새로운 sequence를 만들어주는 function\n",
    "            new_sequence = list()\n",
    "            for i, layer in enumerate(self.sequnece):\n",
    "                if i == layer_index:\n",
    "                    new_sequence.append(new_layer)\n",
    "                else:\n",
    "                    new_sequence.append(layer)\n",
    "            return new_sequence\n",
    "\n",
    "        def eval_sequence(x, sequence): # 새로 만든 sequence로 평가를 하는 function\n",
    "            for layer in sequence:\n",
    "                x = layer(x)\n",
    "            return x\n",
    "        \n",
    "    \n",
    "        loss = loss_func(self(x), y)\n",
    "\n",
    "        for layer_id, layer in enumerate(self.sequence):\n",
    "            for w_i, w in enumerate(layer.W):\n",
    "                for w_j, ww in enumerate(w): # 행과 열 모두 iteration\n",
    "                    W = np.copy(layer.W)\n",
    "                    W[w_i][w_j] = ww + epsilon\n",
    "\n",
    "                    new_layer = Dense(W, layer.b, layer.a)\n",
    "                    new_seq = get_new_sequence(layer_id, new_layer)\n",
    "                    h = eval_sequence(x, new_seq)\n",
    "                    num_grad = (loss_func(h, y) - loss) / epsilon\n",
    "                    # Numerical Gradients : (f(x + eps) - f(x)) / eps\n",
    "                    layer.dW[w_i][w_j] = num_grad\n",
    "                    \n",
    "            for b_i, bb in enumerate(layer.b): \n",
    "                # bias는 vector이기 때문에 iteration은 1번만 돌려도 됨.\n",
    "                b = np.copy(layer.b)\n",
    "                b[b_i] = bb + epsilon\n",
    "\n",
    "                new_layer = Dense(layer.W, b, layer.a)\n",
    "                new_seq = get_new_sequence(layer_id, new_layer)\n",
    "                h = eval_sequence(x, new_seq)\n",
    "                num_grad = (loss_func(h, y) - loss) / epsilon\n",
    "                # Numerical Gradients : (f(x + eps) - f(x)) / eps\n",
    "                layer.db[b_i] = num_grad\n",
    "              \n",
    "\n",
    "        return loss'''"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 49,
   "outputs": []
  },
  {
   "source": [
    "- 제대로 동작하지 않아 다른 함수 정의"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, hidden_depth, num_neuron, input, output, activation=sigmoid):\n",
    "        def init_var(i, o):\n",
    "            return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))\n",
    "\n",
    "        self.sequence = list()\n",
    "        # First hidden layer\n",
    "        W, b = init_var(input, num_neuron)\n",
    "        self.sequence.append(Dense(W, b, activation))\n",
    "\n",
    "        # Hidden Layers\n",
    "        for index in range(hidden_depth):\n",
    "            W, b = init_var(num_neuron, num_neuron)\n",
    "            self.sequence.append(Dense(W, b, activation))\n",
    "\n",
    "        # Output Layer\n",
    "        W, b = init_var(num_neuron, output)\n",
    "        self.sequence.append(Dense(W, b, activation))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def calc_gradient(self, loss_obj):\n",
    "        loss_obj.dh = loss_obj.grad()\n",
    "        self.sequence.append(loss_obj)\n",
    "        \n",
    "        # back-prop loop\n",
    "        for i in range(len(self.sequence) - 1, 0, -1):\n",
    "            l1 = self.sequence[i]\n",
    "            l0 = self.sequence[i - 1]\n",
    "            \n",
    "            l0.dh = _m(l0.grad(), l1.dh)\n",
    "            l0.dW = l0.grad_W(l1.dh)\n",
    "            l0.db = l0.grad_b(l1.dh)\n",
    "        \n",
    "        self.sequence.remove(loss_obj)"
   ]
  },
  {
   "source": [
    "### 정리\n",
    "- 1) Numerical Gradient를 구하기 위해서는 기준이 되는 loss를 구해야함.\n",
    "- 2) 각각의 학습을 하고자하는 스칼라들을 epsilon만큼 옮겨서 loss function을 새로 구하고,\n",
    "- 3) 기준이 되는 loss function이랑 차이를 구해준다.\n",
    "- 4) epsilon 만큼 이동하였으니 epsilon으로 나눠주면, Numerical Gradient 정의에 의해서 구하게 되고 해당 스칼라 위치에 Gradient 저장"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 경사하강 학습법"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n",
    "    loss = loss_obj(network(x), y)  # Forward inference\n",
    "    network.calc_gradient(loss_obj)  # Back-propagation\n",
    "    for layer in network.sequence:\n",
    "        layer.W += -alpha * layer.dW\n",
    "        layer.b += -alpha * layer.db\n",
    "    return loss"
   ]
  },
  {
   "source": [
    "### 동작 테스트"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0: Test loss 0.2047870444406289\nEpoch 1: Test loss 0.2024852265882107\nEpoch 2: Test loss 0.2002110775603565\nEpoch 3: Test loss 0.1979650190753927\nEpoch 4: Test loss 0.19574742495064051\nEpoch 5: Test loss 0.19355862200604426\nEpoch 6: Test loss 0.19139889111581532\nEpoch 7: Test loss 0.1892684683933229\nEpoch 8: Test loss 0.18716754649436362\nEpoch 9: Test loss 0.185096276024002\nEpoch 10: Test loss 0.183054767032393\nEpoch 11: Test loss 0.18104309058534576\nEpoch 12: Test loss 0.17906128039585895\nEpoch 13: Test loss 0.17710933450342906\nEpoch 14: Test loss 0.17518721698858827\nEpoch 15: Test loss 0.17329485971085162\nEpoch 16: Test loss 0.17143216405903\nEpoch 17: Test loss 0.16959900270367623\nEpoch 18: Test loss 0.1677952213422702\nEpoch 19: Test loss 0.16602064042859202\nEpoch 20: Test loss 0.16427505687858135\nEpoch 21: Test loss 0.16255824574581576\nEpoch 22: Test loss 0.16086996186055466\nEpoch 23: Test loss 0.1592099414270889\nEpoch 24: Test loss 0.15757790357488943\nEpoch 25: Test loss 0.15597355185976977\nEpoch 26: Test loss 0.15439657571195364\nEpoch 27: Test loss 0.15284665182857604\nEpoch 28: Test loss 0.1513234455087292\nEpoch 29: Test loss 0.14982661192971375\nEpoch 30: Test loss 0.1483557973636427\nEpoch 31: Test loss 0.14691064033400167\nEpoch 32: Test loss 0.1454907727121653\nEpoch 33: Test loss 0.144095820754234\nEpoch 34: Test loss 0.1427254060788692\nEpoch 35: Test loss 0.1413791465870825\nEpoch 36: Test loss 0.14005665732517283\nEpoch 37: Test loss 0.1387575512922096\nEpoch 38: Test loss 0.13748144019362818\nEpoch 39: Test loss 0.1362279351426458\nEpoch 40: Test loss 0.1349966473113156\nEpoch 41: Test loss 0.13378718853312405\nEpoch 42: Test loss 0.13259917185909906\nEpoch 43: Test loss 0.13143221206944006\nEpoch 44: Test loss 0.13028592614270296\nEpoch 45: Test loss 0.12915993368458306\nEpoch 46: Test loss 0.12805385731833027\nEpoch 47: Test loss 0.1269673230388141\nEpoch 48: Test loss 0.12589996053222324\nEpoch 49: Test loss 0.12485140346334987\nEpoch 50: Test loss 0.12382128973235715\nEpoch 51: Test loss 0.122809261702881\nEpoch 52: Test loss 0.12181496640325523\nEpoch 53: Test loss 0.12083805570258968\nEpoch 54: Test loss 0.11987818646336562\nEpoch 55: Test loss 0.11893502067214476\nEpoch 56: Test loss 0.11800822554992195\nEpoch 57: Test loss 0.11709747364357934\nEpoch 58: Test loss 0.1162024428998346\nEpoch 59: Test loss 0.11532281672300283\nEpoch 60: Test loss 0.1144582840178265\nEpoch 61: Test loss 0.11360853921855957\nEpoch 62: Test loss 0.11277328230542648\nEpoch 63: Test loss 0.11195221880951332\nEpoch 64: Test loss 0.11114505980708594\nEpoch 65: Test loss 0.11035152190426988\nEpoch 66: Test loss 0.10957132721297053\nEpoch 67: Test loss 0.10880420331885385\nEpoch 68: Test loss 0.10804988324215782\nEpoch 69: Test loss 0.10730810539205166\nEpoch 70: Test loss 0.10657861351521218\nEpoch 71: Test loss 0.10586115663924131\nEpoch 72: Test loss 0.1051554890115032\nEpoch 73: Test loss 0.1044613700339196\nEpoch 74: Test loss 0.10377856419422171\nEpoch 75: Test loss 0.10310684099412024\nEpoch 76: Test loss 0.10244597487481998\nEpoch 77: Test loss 0.10179574514027251\nEpoch 78: Test loss 0.10115593587852963\nEpoch 79: Test loss 0.1005263358815309\nEpoch 80: Test loss 0.09990673856363143\nEpoch 81: Test loss 0.09929694187915065\nEpoch 82: Test loss 0.09869674823919869\nEpoch 83: Test loss 0.09810596442801466\nEpoch 84: Test loss 0.09752440151903102\nEpoch 85: Test loss 0.09695187479085722\nEpoch 86: Test loss 0.09638820364335982\nEpoch 87: Test loss 0.09583321151399755\nEpoch 88: Test loss 0.09528672579455577\nEpoch 89: Test loss 0.09474857774840899\nEpoch 90: Test loss 0.09421860242842786\nEpoch 91: Test loss 0.0936966385956335\nEpoch 92: Test loss 0.09318252863869228\nEpoch 93: Test loss 0.09267611849433065\nEpoch 94: Test loss 0.09217725756874376\nEpoch 95: Test loss 0.09168579866005896\nEpoch 96: Test loss 0.09120159788190896\nEpoch 97: Test loss 0.09072451458816194\nEpoch 98: Test loss 0.0902544112988476\nEpoch 99: Test loss 0.08979115362731295\n0.06917071342468262 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0.0, 1.0, (10,))\n",
    "y = np.random.normal(0.0, 1.0, (2,))\n",
    "\n",
    "t = time.time()\n",
    "dnn = DNN(hidden_depth=5, num_neuron=32, input=10, output=2, activation=Sigmoid)\n",
    "loss_obj = MeanSquaredError()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn, x, y, loss_obj, alpha=0.01)\n",
    "    print('Epoch {}: Test loss {}'.format(epoch, loss))\n",
    "print('{} seconds elapsed.'.format(time.time() - t))"
   ]
  },
  {
   "source": [
    "### Hidden_depth를 10으로 증가시켜보자"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0: Test loss 1.332709435842509\nEpoch 1: Test loss 1.3177567856052912\nEpoch 2: Test loss 1.3030070049852824\nEpoch 3: Test loss 1.2884824351838535\nEpoch 4: Test loss 1.2742033237473076\nEpoch 5: Test loss 1.2601877066044778\nEpoch 6: Test loss 1.2464513388166418\nEpoch 7: Test loss 1.233007671788581\nEpoch 8: Test loss 1.219867873198329\nEpoch 9: Test loss 1.2070408847720233\nEpoch 10: Test loss 1.194533512277451\nEpoch 11: Test loss 1.1823505417228335\nEpoch 12: Test loss 1.1704948756894162\nEpoch 13: Test loss 1.158967683944489\nEpoch 14: Test loss 1.1477685629137768\nEpoch 15: Test loss 1.136895699175177\nEpoch 16: Test loss 1.1263460328095978\nEpoch 17: Test loss 1.1161154171565775\nEpoch 18: Test loss 1.106198772229416\nEpoch 19: Test loss 1.096590229713919\nEpoch 20: Test loss 1.0872832680838163\nEpoch 21: Test loss 1.0782708369006286\nEpoch 22: Test loss 1.069545469819889\nEpoch 23: Test loss 1.0610993861987157\nEpoch 24: Test loss 1.05292458149559\nEpoch 25: Test loss 1.0450129068785012\nEpoch 26: Test loss 1.037356138620697\nEpoch 27: Test loss 1.029946037973198\nEpoch 28: Test loss 1.022774402268982\nEpoch 29: Test loss 1.0158331080439753\nEpoch 30: Test loss 1.0091141469623799\nEpoch 31: Test loss 1.0026096553153352\nEpoch 32: Test loss 0.9963119378281904\nEpoch 33: Test loss 0.9902134864676696\nEpoch 34: Test loss 0.9843069948897618\nEpoch 35: Test loss 0.9785853691154113\nEpoch 36: Test loss 0.9730417349663529\nEpoch 37: Test loss 0.9676694427394981\nEpoch 38: Test loss 0.9624620695463691\nEpoch 39: Test loss 0.9574134196950955\nEpoch 40: Test loss 0.9525175234468971\nEpoch 41: Test loss 0.9477686344371237\nEpoch 42: Test loss 0.9431612260128588\nEpoch 43: Test loss 0.9386899867048135\nEpoch 44: Test loss 0.934349815020576\nEpoch 45: Test loss 0.9301358137190733\nEpoch 46: Test loss 0.9260432837020688\nEpoch 47: Test loss 0.9220677176374537\nEpoch 48: Test loss 0.9182047934106584\nEpoch 49: Test loss 0.9144503674845308\nEpoch 50: Test loss 0.9108004682341553\nEpoch 51: Test loss 0.9072512893111784\nEpoch 52: Test loss 0.9037991830819421\nEpoch 53: Test loss 0.9004406541749922\nEpoch 54: Test loss 0.8971723531660662\nEpoch 55: Test loss 0.8939910704223544\nEpoch 56: Test loss 0.8908937301224966\nEpoch 57: Test loss 0.8878773844643006\nEpoch 58: Test loss 0.8849392080684283\nEpoch 59: Test loss 0.8820764925831779\nEpoch 60: Test loss 0.8792866414929402\nEpoch 61: Test loss 0.8765671651307816\nEpoch 62: Test loss 0.8739156758939116\nEpoch 63: Test loss 0.8713298836594121\nEpoch 64: Test loss 0.8688075913965116\nEpoch 65: Test loss 0.8663466909708385\nEpoch 66: Test loss 0.8639451591354319\nEpoch 67: Test loss 0.8616010537028067\nEpoch 68: Test loss 0.8593125098920255\nEpoch 69: Test loss 0.8570777368445016\nEpoch 70: Test loss 0.8548950143021244\nEpoch 71: Test loss 0.8527626894412514\nEpoch 72: Test loss 0.8506791738561085\nEpoch 73: Test loss 0.8486429406852212\nEpoch 74: Test loss 0.8466525218745823\nEpoch 75: Test loss 0.8447065055714131\nEpoch 76: Test loss 0.8428035336425213\nEpoch 77: Test loss 0.8409422993114511\nEpoch 78: Test loss 0.8391215449087994\nEpoch 79: Test loss 0.8373400597302806\nEpoch 80: Test loss 0.8355966779973293\nEpoch 81: Test loss 0.8338902769152342\nEpoch 82: Test loss 0.8322197748240127\nEpoch 83: Test loss 0.8305841294374392\nEpoch 84: Test loss 0.8289823361658473\nEpoch 85: Test loss 0.827413426518529\nEpoch 86: Test loss 0.8258764665817482\nEpoch 87: Test loss 0.8243705555685757\nEpoch 88: Test loss 0.8228948244369405\nEpoch 89: Test loss 0.8214484345724669\nEpoch 90: Test loss 0.8200305765328394\nEpoch 91: Test loss 0.8186404688506022\nEpoch 92: Test loss 0.8172773568914522\nEpoch 93: Test loss 0.815940511765245\nEpoch 94: Test loss 0.8146292292870633\nEpoch 95: Test loss 0.8133428289858466\nEpoch 96: Test loss 0.8120806531582013\nEpoch 97: Test loss 0.8108420659651384\nEpoch 98: Test loss 0.8096264525696064\nEpoch 99: Test loss 0.808433218312791\n0.09454584121704102 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0.0, 1.0, (10,))\n",
    "y = np.random.normal(0.0, 1.0, (2,))\n",
    "\n",
    "t = time.time()\n",
    "dnn = DNN(hidden_depth=10, num_neuron=32, input=10, output=2, activation=Sigmoid)\n",
    "loss_obj = MeanSquaredError()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn, x, y, loss_obj, alpha=0.01)\n",
    "    print('Epoch {}: Test loss {}'.format(epoch, loss))\n",
    "print('{} seconds elapsed.'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}