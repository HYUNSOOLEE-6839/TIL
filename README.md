# Today I Learned
<br>

## Day-01
- Study and Summary of Python's Basic Grammar and Calculations.

## Day-02
- **Studied and Summarized**
1. Array programming through the *Numpy-Package*
2. Data_Analysis through the *Pandas-Package*

## Day-03
- **Studied and Summarized**
1. Data-Visualization with *Matplotlib*
2. Prediction of Iris Variety by Basic-Machine-Learning Techniques

## Day-04
- **Studied and Summarize**
1. Found missing data through the *Basic-package(MissingNo)* of data preprocessing
2. learned about the document preprocessing capabilities through the basic methods of *scikit-learn*
3. studied Tokenizer to Wordcloud through the *NLTK* natural language preprocessing package.

### From today,
I am planning to study from the beginning thorugh a new lecture instead of reviewing what I have learned before. üòÅüòÅ
- **Basic_Concepts_01** : The study of Python's *basic concepts* and strings, lists and tuples and dictionaries.
- **Basic_Concepts_02** : Studying *Conditional* and *Repeated Statements*.

## Day-05
- **Basic_Concepts_03** : Definition and use of *Functions*, and study of parameters, *Lambda Functions*
- **Basic_Concepts_04** : Studied various functions implemented within *Module* and studied *class, object and method.* Among them, I studied *Class Inheritance, method override, etc.* in detail.
- **Basic_Concepts_05** : I studied the *Regular Expression*, and found the pattern through various methods such as meta-character and minimum-matching.

## Day-06
- Used *BeautifulSoup4* to crawling the web such as the title and date of the Internet news article.

## Day-07
- Studied Web-Page crawl using *Selenium* and crawled on several pages including Automative-Login and Automative-Searching.
- The operating system that I used to study crawling was Window OS, and I felt that there was a small difference when I used MAC OS this time, and it was a good opportunity to fix the code one by one, not by searching.

## Day-08
- I studied how to handle multi-dimensional data usingn the Numpy and how to change or slicing these data into different dimensions. In particular, the concept of the axis was clearly established, and it was realized that it could be used in various fields or in various fields, and that it was simpler and more convenient to operate a computer by aligning shapes with broadcasting functions.
- Furthermore, using the basic Matplotlib, I was able to graph my defined fuctions or results as desired, and I was able to draw graphs more neatly and easily recognizable through styling each graph.

## Day-09
- Through the Pandas module, I study how to leverage Series and DataFrame to organize, reconstruct, and classify the given data easily.
- Future learning plans will be used in Machine-Learning based on what has been learned and reviewed so far, and I have established a basic concept of what is in Machine-Learning.

## Day-10
- I looked at the types of machine learning, and we also looked at what role each model plays, and what kinds of deep learning exist.
What was impressive was that "the fourth industrial revolution that is happening now" by looking at various techniques such as improving the picture quality or changing the day and night of the background through Deep Learning's "GAN" model.
- It studied the conformity assessment and experimental design process of the model, and established the concept of 'overfitting'.

## Day-11
- I studied the basis of 'statistics' that will be used in whether it is machine learning or deep learning, which were statistical probabilities and distribution, statistical reasoning, and tests.  which were difficult to understand, but I was able to establish concepts by applying them in real life as an example.

## Day-12
- I studied the mathematical concepts of differentiation and matrix, simple regression analysis. Among them,  Tested and estimated the regression coefficient, and practiced simple linear regression with Boston's house price as an example.

## Day-13
- Using Boston housing price data as an example, we studied multiple linear regression analysis, including crime rate, number of rooms per house, and ratio of lower classes in the population through three variables.
- As a result, the overall model had the smallest residuals, each variable had a low p-value, and the rate of variation of y described as three variables overlapped.
In other words, we show that the regression model I used is not bad to use even though it is multi-communication.

## Day-14
- Multilinear regression analysis was conducted through data from Toyota car prices and Boston house prices, and the contents included how to diagnose multicollinearity, regression diagnosis, regression model determination, and variable selection methods.
- The contents are organized while studying multi-linear regression analysis. 
1) Data is pre-processed. 
2) Train the model.
3) The p-value of the R-square or variable shows that 'this model has some performance'.
4) Based on domain knowledge, determine which variables to remove after checking the variables.
5) While checking multicollinearity through coordination matrix or VIF, recognize that 'this variable also needs to be erased'. If there are one or two variables to erase, you can erase them daily, but if there are many, use variable selection method.
6) While checking the response, check that the model is well-suited to a certain extent.
7) Be sure to check the performance for validation_data and select a model.

- Practiced logistic regression through Personal Loan dataset.

## Day-15
- I studied how to reduce the regression coefficient through Ridge and Lasso, and studied dimension reduction through PCA.
- After studying regression analysis, I started to study machine learning in earnest. In its contents, we study Naive-Bayes models and study KNN. While studying KNN, we considered solving the overfitting problem through Cross-Validation, and we were able to establish the concept through various materials. Furthermore, while practicing on KNNs, we experienced in-house increasing the performance of KNNs in certain situations through methods that give less weight even if they are in the same Neighbor, and gave us an opportunity to consider how to improve accuracy through multiple techniques.

## Day-16
- Until today, I have completed basic studies of basic machine learning models, including LDA, QDA, SVM, neural network models, and decision trees.
- The Iris data were classified through decision trees based on Sepal_length and Sepal_width, and the difference in accuracy was determined by giving different Depths for each classification.
- While studying machine learning, there was a change in accuracy or significance depending on how each variable or parameter was designated, and it was possible to pick out data discretization or wrong data through coding without direct calculation, and these processes were able to learn the technology to be honest and accurate.
- From tomorrow, I will study 'ensemble' which uses a mix of these techniques.

## Day-17
- Today, we studied 'ensemble', a concept that utilizes several basic models to create one new model.
- Using single tree-based models, I changed the number of sampling, and verified with post-fitting evaluation data for decision tree models, showing better accuracy.
- The most interesting study, "Random Forest," showed that performance, like any other model, did not improve any more, and coding showed that it could achieve 100% accuracy with proper sampling and depth.
- The question here was, 'Why not use a more complex model of deep learning to predict it would be more accurate?', and the answer was to coding myself, but machine learning, a relatively simple technique, was more accurate. We realized that simple models have high accuracy even with simple techniques, and complex techniques can produce poor results.'
- In other words, it can be easily likened to 'using grenades to catch mice?'
- While studying machine learning and ensembles, the most important thing in improving accuracy is Feature_selection, which laid the foundation for thinking about what techniques and models should be used for each data and type.

